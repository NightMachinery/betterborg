#+TITLE: betterborg/breadcrumbs

* Delete all cached files (=history_util=) from Redis:
#+begin_src zsh :eval never
redis-cli --scan --pattern 'borg:files:*' | xargs -r redis-cli DEL
#+end_src

* _
#+begin_verse
llm_chat:

```
Traceback (most recent call last):
File "/home/eva/code/betterborg/llm_chat_plugins/llm_chat.py", line 3634, in
chat_handler
    response_text, has_image = await _handle_native_gemini_image_generation(
File "/home/eva/code/betterborg/llm_chat_plugins/llm_chat.py", line 758, in 
_handle_native_gemini_image_generation
    for chunk in client.models.generate_content_stream(
File "/home/eva/micromamba/envs/p310/lib/python3.10/site-packages/google/gen
ai/models.py", line 6290, in generate_content_stream
    for chunk in response:
File "/home/eva/micromamba/envs/p310/lib/python3.10/site-packages/google/gen
ai/models.py", line 5041, in _generate_content_stream
    for response in self._api_client.request_streamed(
File "/home/eva/micromamba/envs/p310/lib/python3.10/site-packages/google/gen
ai/_api_client.py", line 1283, in request_streamed
    session_response = self._request(http_request, http_options, stream=True)
File "/home/eva/micromamba/envs/p310/lib/python3.10/site-packages/google/gen
ai/_api_client.py", line 1086, in _request
    return self._retry(self._request_once, http_request, stream)  # type: 
ignore[no-any-return]
File "/home/eva/micromamba/envs/p310/lib/python3.10/site-packages/tenacity/_
_init__.py", line 477, in __call__
    do = self.iter(retry_state=retry_state)
File "/home/eva/micromamba/envs/p310/lib/python3.10/site-packages/tenacity/_
_init__.py", line 378, in iter
    result = action(retry_state)
File "/home/eva/micromamba/envs/p310/lib/python3.10/site-packages/tenacity/_
_init__.py", line 420, in exc_check
    raise retry_exc.reraise()
File "/home/eva/micromamba/envs/p310/lib/python3.10/site-packages/tenacity/_
_init__.py", line 187, in reraise
    raise self.last_attempt.result()
File 
"/home/eva/micromamba/envs/p310/lib/python3.10/concurrent/futures/_base.py", 
line 451, in result
    return self.__get_result()
File 
"/home/eva/micromamba/envs/p310/lib/python3.10/concurrent/futures/_base.py", 
line 403, in __get_result
    raise self._exception
File "/home/eva/micromamba/envs/p310/lib/python3.10/site-packages/tenacity/_
_init__.py", line 480, in __call__
    result = fn(*args, **kwargs)
File "/home/eva/micromamba/envs/p310/lib/python3.10/site-packages/google/gen
ai/_api_client.py", line 1051, in _request_once
    errors.APIError.raise_for_response(response)
File "/home/eva/micromamba/envs/p310/lib/python3.10/site-packages/google/gen
ai/errors.py", line 105, in raise_for_response
    raise ClientError(status_code, response_json, response)
google.genai.errors.ClientError: 404 NOT_FOUND. {'error': {'code': 404, 
'message': 'models/gemini-2.0-flash-exp-image-generation is not found for API 
version v1beta, or is not supported for generateContent. Call ListModels to 
see the list of available models and their supported methods.', 'status': 
'NOT_FOUND'}}
```


I am not sure why this error happens. This code example works:
```
# To run this code you need to install the following dependencies:
# pip install google-genai

import base64
import mimetypes
import os
from google import genai
from google.genai import types


def save_binary_file(file_name, data):
    f = open(file_name, "wb")
    f.write(data)
    f.close()
    print(f"File saved to to: {file_name}")


def generate():
    client = genai.Client(
        api_key=os.environ.get("GEMINI_API_KEY"),
    )

    model = "gemini-2.0-flash-preview-image-generation"
    contents = [
        types.Content(
            role="user",
            parts=[
                types.Part.from_text(text=\"""draw a unicorn\"""),
            ],
        ),
    ]
    generate_content_config = types.GenerateContentConfig(
        response_modalities=[
            "IMAGE",
            "TEXT",
        ],
    )

    file_index = 0
    for chunk in client.models.generate_content_stream(
        model=model,
        contents=contents,
        config=generate_content_config,
    ):
        if (
            chunk.candidates is None
            or chunk.candidates[0].content is None
            or chunk.candidates[0].content.parts is None
        ):
            continue
        if chunk.candidates[0].content.parts[0].inline_data and 
chunk.candidates[0].content.parts[0].inline_data.data:
            file_name = f"ENTER_FILE_NAME_{file_index}"
            file_index += 1
            inline_data = chunk.candidates[0].content.parts[0].inline_data
            data_buffer = inline_data.data
            file_extension = mimetypes.guess_extension(inline_data.mime_type)
            save_binary_file(f"{file_name}{file_extension}", data_buffer)
        else:
            print(chunk.text)

if __name__ == "__main__":
    generate()
```
#+end_verse

** [[https://stackoverflow.com/questions/79581891/gemini-flash-image-generation-not-working-gemini-2-0-flash-exp][google api - Gemini Flash Image Generation not working (gemini-2.0-flash-exp) - Stack Overflow]]
#+begin_verse
According to the [[https://ai.google.dev/gemini-api/docs/models][official statement]], "gemini-2.0-flash-exp-image-generation is not currently supported in a number of countries in Europe, Middle East & Africa".

If you're geo-blocked, you options are:

- Use a VPN

- Use a third party provider like [[https://ir.myqa.cc/][Image Router]]
#+end_verse

Indeed, this is the case. This fails:
#+begin_src zsh :eval never
GEMINI_API_KEY=$gemini_api_key pxa98 python a.py
#+end_src

While this works:
#+begin_src zsh :eval never
GEMINI_API_KEY=$gemini_api_key pxa python a.py
#+end_src

#+begin_src python :eval never
# To run this code you need to install the following dependencies:
# pip install google-genai

import base64
import mimetypes
import os
from google import genai
from google.genai import types


def save_binary_file(file_name, data):
    f = open(file_name, "wb")
    f.write(data)
    f.close()
    print(f"File saved to to: {file_name}")


def generate():
    client = genai.Client(
        api_key=os.environ.get("GEMINI_API_KEY"),
    )

    model = "gemini-2.0-flash-preview-image-generation"
    contents = [
        types.Content(
            role="user",
            parts=[
                types.Part.from_text(text="""draw a unicorn"""),
            ],
        ),
    ]
    generate_content_config = types.GenerateContentConfig(
        response_modalities=[
            "IMAGE",
            "TEXT",
        ],
    )

    file_index = 0
    for chunk in client.models.generate_content_stream(
        model=model,
        contents=contents,
        config=generate_content_config,
    ):
        if (
            chunk.candidates is None
            or chunk.candidates[0].content is None
            or chunk.candidates[0].content.parts is None
        ):
            continue
        if chunk.candidates[0].content.parts[0].inline_data and chunk.candidates[0].content.parts[0].inline_data.data:
            file_name = f"ENTER_FILE_NAME_{file_index}"
            file_index += 1
            inline_data = chunk.candidates[0].content.parts[0].inline_data
            data_buffer = inline_data.data
            file_extension = mimetypes.guess_extension(inline_data.mime_type)
            save_binary_file(f"{file_name}{file_extension}", data_buffer)
        else:
            print(chunk.text)

if __name__ == "__main__":
    generate()
#+end_src
